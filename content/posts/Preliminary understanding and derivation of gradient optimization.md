---
title: "梯度优化初步理解与推导"
date: 2024-03-21
draft: false
description: "从单变量到多变量函数，深入浅出地讲解梯度优化原理，包含数学推导与直观解释"
tags: ["机器学习", "数学", "优化"]
categories: ["技术"]
author: "徐亚飞"
ShowToc: true
TocOpen: true
weight: 2
---

## 梯度优化初步理解与推导

在训练机器学习模型时，我们经常会遇到一个核心任务：**最小化一个损失函数**。梯度优化就是最常见的一种优化方法，其中最基本的就是**梯度下降法（Gradient Descent）**。本文从最简单的单变量情况出发，逐步介绍梯度优化的基本概念，并通过推导来解释为什么变量的更新形式是"减去导数"。

---

### 一、单变量函数的梯度优化

我们先考虑最简单的一维情形：目标是最小化一个单变量函数 \( f(x) \)。

假设当前我们在位置 \( x \)，我们想要往某个方向移动一点，以期降低函数值。最直观的想法是，函数的导数（梯度）可以告诉我们函数在该点的变化趋势。

如果导数为正，说明函数在这里是上升的，那么我们应该往左（负方向）走；如果导数为负，说明函数是下降的，我们应该往右（正方向）走。

因此，我们更新变量的方向应当**与导数方向相反**。如果步长设置为 \( \eta \)（称为**学习率**），那么更新公式就是：

\[
x_{\text{new}} = x_{\text{old}} - \eta \cdot f'(x_{\text{old}})
\]

---

### 二、学习率的引入

学习率 \( \eta \) 控制着我们每一步走多远。如果 \( \eta \) 太小，虽然方向对了，但前进非常缓慢，优化过程会很慢；如果 \( \eta \) 太大，可能会越过最小值，甚至震荡不收敛。

因此，选择合适的学习率非常关键。实际应用中可能会使用固定学习率、动态调整学习率、甚至使用不同优化器（如 Adam）来改进这个过程。

---

### 三、为什么变量更新要"减去"导数？——从数学推导理解梯度下降

很多初学者会疑惑：为什么变量更新的公式是"减去"导数，而不是加上？这一节我们从泰勒展开和最优化理论的角度来做一个严格推导。

#### 3.1 优化目标：寻找最小值

我们希望寻找使函数 \( f(x) \) 最小的点。假设当前我们在某一点 \( x \)，下一步想往某个方向 \( d \) 走一小步（大小为 \( \alpha \)）。新的点就是：

\[
x_{\text{new}} = x + \alpha d
\]

#### 3.2 用一阶泰勒展开近似函数变化

考虑函数 \( f(x + \alpha d) \) 在点 \( x \) 附近的变化情况，一阶泰勒展开为：

\[
f(x + \alpha d) \approx f(x) + \alpha f'(x) \cdot d
\]

我们希望让函数值变小，也就是说希望：

\[
f(x + \alpha d) < f(x) \Rightarrow \alpha f'(x) \cdot d < 0
\]

由于 \( \alpha > 0 \)，这个不等式可以简化为：

\[
f'(x) \cdot d < 0
\]

说明我们要选择一个方向 \( d \)，使得它和导数方向相反。

#### 3.3 最速下降方向是负梯度方向

那么，哪个方向能让函数下降得最快呢？答案是：

> 当 \( d = -f'(x) \) 时，函数下降最快，这就是最速下降法中的结论。

于是，我们更新变量时就取这个方向：

\[
x_{\text{new}} = x + \alpha \cdot d = x - \alpha \cdot f'(x)
\]

我们将 \( \alpha \) 改名为学习率 \( \eta \)，得到常用的梯度下降更新公式：

\[
x_{\text{new}} = x - \eta \cdot f'(x)
\]

---

### 四、示例：最小化一个简单函数

考虑函数 \( f(x) = (x - 2)^2 \)，它的导数是 \( f'(x) = 2(x - 2) \)。

假设从 \( x_0 = 0 \) 开始，学习率为 \( \eta = 0.1 \)，那么：

\[
x_1 = x_0 - 0.1 \cdot f'(x_0) = 0 - 0.1 \cdot (-4) = 0.4
\]

继续迭代下去，就可以不断逼近函数的最小值 2。

---

### 五、多变量函数的梯度优化

当我们面对的是一个多变量函数 \( f(x, y) \) 时，梯度优化依然成立。此时函数的梯度是一个向量：

\[
\nabla f(x, y) = \left[ \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right]
\]

更新规则就变为：

\[
\begin{aligned}
x_{\text{new}} &= x - \eta \cdot \frac{\partial f}{\partial x} \\
y_{\text{new}} &= y - \eta \cdot \frac{\partial f}{\partial y}
\end{aligned}
\]

在实际操作中，由于函数形状可能非常复杂（例如鞍点、局部极小值、陡峭或平缓区域等），多变量的梯度下降容易出现如下问题：

- 陷入局部最小值或鞍点，而非全局最优；
- 在某些方向上下降缓慢（例如"峡谷"型函数）；
- 对初始点非常敏感。

#### 多变量优化的可视化理解

在二维函数中，梯度优化的路径可以在等高线图（contour plot）上直观表现出来。

函数 \( f(x, y) \) 的等高线图是一个二维平面，其中每条曲线表示函数值相同的点。梯度下降的每一步都会沿着垂直于等高线、指向函数值减小的方向移动。

换句话说：

> 每一步的梯度方向都是当前等高线的法向量，下降轨迹会"穿越"这些等高线向最低点收敛。

因此，观察一个函数的梯度下降路径，不仅可以判断收敛是否合理，还可以帮助我们理解学习率是否合适（太大会震荡、太小会缓慢）。

---

### 六、小结

从数学推导可以看出，"变量减去导数"并不是拍脑袋的经验做法，而是有严密的逻辑：

- 函数值想要下降，变量必须朝着导数的反方向移动；
- 负导数方向是函数下降最快的方向；
- 因此，变量更新应当是：

\[
x \leftarrow x - \eta \cdot f'(x)
\]

这个公式，就是梯度优化的起点。
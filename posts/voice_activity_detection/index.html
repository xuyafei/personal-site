<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>语音活动检测（VAD）技术详解 | 我的博客</title>
<meta name=keywords content="音频处理,VAD,WebRTC,信号处理,机器学习"><meta name=description content="深入解析语音活动检测（VAD）的原理、实现和应用，包括 WebRTC VAD 的实现细节"><meta name=author content="徐亚飞"><link rel=canonical href=https://xuyafei.github.io/personal-site/posts/voice_activity_detection/><link crossorigin=anonymous href=/personal-site/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://xuyafei.github.io/personal-site/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://xuyafei.github.io/personal-site/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://xuyafei.github.io/personal-site/favicon-32x32.png><link rel=apple-touch-icon href=https://xuyafei.github.io/personal-site/apple-touch-icon.png><link rel=mask-icon href=https://xuyafei.github.io/personal-site/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://xuyafei.github.io/personal-site/posts/voice_activity_detection/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAYQHy+K8nqKdr1EzvFzSQC+TAXx6gNQgoRxKtj+P9vvCCQTRWiV crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!1})})</script><script>MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta property="og:url" content="https://xuyafei.github.io/personal-site/posts/voice_activity_detection/"><meta property="og:site_name" content="我的博客"><meta property="og:title" content="语音活动检测（VAD）技术详解"><meta property="og:description" content="深入解析语音活动检测（VAD）的原理、实现和应用，包括 WebRTC VAD 的实现细节"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-22T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-22T00:00:00+00:00"><meta property="article:tag" content="音频处理"><meta property="article:tag" content="VAD"><meta property="article:tag" content="WebRTC"><meta property="article:tag" content="信号处理"><meta property="article:tag" content="机器学习"><meta name=twitter:card content="summary"><meta name=twitter:title content="语音活动检测（VAD）技术详解"><meta name=twitter:description content="深入解析语音活动检测（VAD）的原理、实现和应用，包括 WebRTC VAD 的实现细节"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"文章","item":"https://xuyafei.github.io/personal-site/posts/"},{"@type":"ListItem","position":2,"name":"语音活动检测（VAD）技术详解","item":"https://xuyafei.github.io/personal-site/posts/voice_activity_detection/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"语音活动检测（VAD）技术详解","name":"语音活动检测（VAD）技术详解","description":"深入解析语音活动检测（VAD）的原理、实现和应用，包括 WebRTC VAD 的实现细节","keywords":["音频处理","VAD","WebRTC","信号处理","机器学习"],"articleBody":"语音活动检测（VAD）概述 语音活动检测（Voice Activity Detection，VAD）是音频信号处理中的基础模块，用于判断一段音频中是否包含人声。它在实时语音通信、语音识别、音频处理等领域发挥着重要作用。\nVAD 的主要应用场景 1. 消除静音段，降低带宽和计算资源消耗 在 VoIP 等实时语音通信中，VAD 可以：\n停止静音段的音频编码与发送 丢弃静音包或减少帧率 暂停对静音段的播放 发送 Comfort Noise（舒适噪声）包替代静音数据 2. 配合音频处理模块使用 VAD 可以与以下模块协同工作：\n回声消除（AEC）：防止在静音时更新错误的回声模型 噪声抑制（NS）：帮助区分\"语音+噪声\"与\"纯噪声\" 自动增益控制（AGC）：防止对静音或噪声进行放大 3. 触发语音识别（ASR）引擎 VAD 可用于：\n检测语音开始，启动语音识别引擎 检测语音结束，终止识别并提交结果 控制\"有声录音\"功能 4. 优化语音通信 在视频会议等场景中：\n静音压缩（silence compression） 发送 CN（Comfort Noise）包 减少传输帧率 VAD 的基本原理 特征提取 VAD 通常基于以下特征进行判断：\n短时能量（Short-Time Energy） E = \\sum_{n=0}^{N-1} x[n]^2 过零率（Zero-Crossing Rate, ZCR） \\text{ZCR} = \\frac{1}{2N} \\sum_{n=1}^{N} | \\text{sgn}(x[n]) - \\text{sgn}(x[n-1]) | 谱熵（Spectral Entropy） 衡量信号频谱的\"有序性\" 语音谱结构复杂，熵值较高 短时谱幅度 语音的频谱幅度分布与噪声不同 VAD 的类型 基于规则的传统 VAD\n算法简单、实时性强 适合嵌入式设备 易受环境噪声影响 基于机器学习的 VAD\n使用统计模型（GMM、HMM） 深度学习模型（LSTM、CNN） 更鲁棒，高噪环境下准确率高 集成在语音处理库中的 VAD\nWebRTC VAD 模块 RNNoise 模块 WebRTC VAD 实现详解 处理流程 分帧与预处理\n10ms/20ms/30ms 帧长 预加重和窗口函数处理 滤波器组分频\n4个频带（80Hz~4kHz） IIR 滤波器进行带通分离 特征计算\n频带能量 谱平坦度 频谱统计量 GMM 判决\np(x) = \\sum_{k=1}^{K} w_k \\cdot \\mathcal{N}(x | \\mu_k, \\Sigma_k) 平滑处理 多帧平滑 动态门限调整 Hangover 机制 实现示例 VadInst* vad; WebRtcVad_Create(\u0026vad); WebRtcVad_Init(vad); WebRtcVad_set_mode(vad, 3); // 模式 0~3，数字越大越敏感 int result = WebRtcVad_Process(vad, 16000, audio_frame, 160); WebRTC VAD 的数学原理与流程详解 步骤一：分帧与预处理 帧划分\n输入音频被分为固定长度帧（10ms/20ms/30ms） 采样率 16kHz 时，10ms = 160 个采样点 预处理\n预加重（Pre-emphasis） 窗口函数（如汉明窗）处理 增强高频特征和时域局部性 步骤二：滤波器组分频 频带划分\n4个频带（80Hz~4kHz） 使用 IIR 滤波器进行带通分离 提取不同频段的能量特征 频带特征\n低频带：80Hz~250Hz 中低频带：250Hz~1kHz 中高频带：1kHz~2kHz 高频带：2kHz~4kHz 步骤三：特征计算 频带能量计算 E_i = \\frac{1}{N} \\sum_{n=0}^{N-1} x_i[n]^2 谱平坦度计算 \\text{Flatness} = \\frac{\\text{几何均值}}{\\text{算术均值}} = \\frac{(\\prod_{i=1}^{N} X_i)^{1/N}}{\\frac{1}{N}\\sum_{i=1}^{N} X_i} 频谱统计量 最大频率分量 频谱质心 频谱带宽 步骤四：GMM 判决模型 GMM 模型结构\n4个语音活动水平类别 每类使用2个高斯分量 特征维度为6 概率计算\n\\log p(x|C_k) = \\log \\sum_{i=1}^M w_{k,i} \\cdot \\mathcal{N}(x; \\mu_{k,i}, \\Sigma_{k,i}) 类别判决 \\hat{k} = \\arg \\max_k \\log p(x|C_k) 步骤五：平滑与判决逻辑 多帧平滑\n3帧或5帧投票法 连续多帧为语音才判定为说话开始 动态门限调整\n根据语音能量动态调整阈值 适应环境变化 Hangover 机制\n说话结束时保持几帧语音状态 防止语尾被截断 VAD 在视频会议中的应用 音频处理链路 麦克风输入 ↓ 前处理：高通滤波 / 去直流偏移 ↓ ► AEC（回声消除） ↓ ► NS（噪声抑制） ↓ ► AGC（自动增益控制） ↓ ► VAD（语音活动检测） ↓ 编码器（Opus / AAC 等） ↓ 网络传输（RTP / SRTP） 各阶段 VAD 应用 与 AEC 协作\n防止静音时更新错误回声模型 避免背景噪声被当作回声源 与 NS 协作\n区分\"语音+噪声\"与\"纯噪声\" 更新噪声模型 与 AGC 协作\n防止对静音或噪声放大 提升听感体验 编码器控制\n判断是否需要编码 控制静音压缩模式 生成舒适噪声（CN） 网络传输控制\n控制 RTP 包发送 调整发送频率 实际应用建议 部署位置\n音频前处理后的某一帧处理阶段 编码前和网络传输控制前 参数调优\n根据实际场景选择合适的灵敏度模式 调整平滑参数减少误判 性能优化\n异步处理避免阻塞 合理设置帧长和缓冲区 总结 VAD 是音频处理中的重要基础模块，通过合理使用 VAD，可以：\n降低带宽和计算资源消耗 提高音频处理质量 优化语音识别效果 改善实时通信体验 后续研究方向 深度学习在 VAD 中的应用 低延迟 VAD 算法研究 多说话人场景的 VAD 噪声环境下的鲁棒性提升 VAD 的数学原理详解 1. 过零率（ZCR）的深入理解 过零率是语音信号分析中最经典的特征之一，它能反映信号的频率特性：\n数学定义 \\text{ZCR} = \\frac{1}{2N} \\sum_{n=1}^{N} | \\text{sgn}(x[n]) - \\text{sgn}(x[n-1]) | 物理意义 高频信号：起伏快，频繁过零（ZCR高） 低频信号：起伏慢，过零次数少（ZCR低） 在 VAD 中的应用 结合能量特征使用： 能量低 + ZCR高 → 可能是噪声 能量高 + ZCR低 → 多数是语音（尤其是元音） 能量高 + ZCR高 → 咝音/辅音/语音边缘 2. 频谱特征分析 短时傅里叶变换（STFT） X(k) = \\sum_{n=0}^{N-1} x(n)w(n)e^{-j2\\pi kn/N} 谱熵计算 H = -\\sum_{i=1}^{N} p_i \\log_2(p_i) 其中 $p_i$ 是第 i 个频带的归一化能量。\n频谱平坦度 \\text{Flatness} = \\frac{\\sqrt[N]{\\prod_{i=1}^{N} X_i}}{\\frac{1}{N}\\sum_{i=1}^{N} X_i} GMM 模型训练详解 1. 数据准备 训练数据收集 语音数据：包含各种说话人、语速、音量的语音 非语音数据：包含各种环境噪声、背景音 数据标注：人工标注语音/非语音段 特征提取 提取 6 维特征向量： 低频能量 中频能量 高频能量 能量比例 谱差 谱平坦度 2. GMM 训练过程 模型初始化 # 示例代码 from sklearn.mixture import GaussianMixture # 初始化 GMM gmm = GaussianMixture( n_components=2, # 每类使用2个高斯分量 covariance_type='diag', # 使用对角协方差矩阵 random_state=0 ) # 训练模型 gmm.fit(X_train) # X_train 是特征矩阵 EM 算法迭代 E 步：计算每个样本属于各个高斯分量的概率 M 步：更新模型参数（均值、协方差、权重） 模型评估 使用验证集评估模型性能 调整模型参数（如高斯分量数量） 3. 实际应用示例 Python 实现示例 import numpy as np from sklearn.mixture import GaussianMixture def extract_features(audio_frame, sample_rate): # 1. 分帧 frame_length = int(0.02 * sample_rate) # 20ms frames = np.array_split(audio_frame, len(audio_frame) // frame_length) features = [] for frame in frames: # 2. 计算频谱 spectrum = np.abs(np.fft.rfft(frame)) # 3. 提取特征 energy = np.sum(frame ** 2) zcr = np.sum(np.abs(np.diff(np.signbit(frame)))) spectral_entropy = -np.sum((spectrum/np.sum(spectrum)) * np.log2(spectrum/np.sum(spectrum) + 1e-10)) features.append([energy, zcr, spectral_entropy]) return np.array(features) def vad_detection(audio_frame, sample_rate, gmm_model): # 1. 特征提取 features = extract_features(audio_frame, sample_rate) # 2. GMM 预测 log_probs = gmm_model.score_samples(features) # 3. 判决 is_speech = log_probs \u003e threshold return is_speech C++ 实现示例（WebRTC 风格） class VAD { public: VAD() { // 初始化 GMM 参数 initGMMParams(); } bool ProcessFrame(const int16_t* audio_frame, int frame_length) { // 1. 特征提取 std::vector\u003cfloat\u003e features = ExtractFeatures(audio_frame, frame_length); // 2. GMM 计算 float log_prob = ComputeGMMProbability(features); // 3. 判决 return log_prob \u003e threshold_; } private: void initGMMParams() { // 初始化 GMM 参数（均值、方差、权重） // 这些参数通常是预训练好的 } std::vector\u003cfloat\u003e ExtractFeatures(const int16_t* frame, int length) { // 实现特征提取 // 返回特征向量 } float ComputeGMMProbability(const std::vector\u003cfloat\u003e\u0026 features) { // 实现 GMM 概率计算 // 返回对数概率 } float threshold_; // GMM 参数 std::vector\u003cfloat\u003e means_; std::vector\u003cfloat\u003e variances_; std::vector\u003cfloat\u003e weights_; }; VAD 性能优化 1. 计算优化 特征计算优化 使用 SIMD 指令加速 预计算常用值 使用查找表代替复杂计算 GMM 计算优化 使用定点数计算 简化协方差矩阵（使用对角矩阵） 预计算常用值 2. 内存优化 缓冲区管理 使用循环缓冲区 避免频繁内存分配 合理设置缓冲区大小 参数存储 使用定点数存储模型参数 压缩存储模型参数 共享常用参数 3. 实时性优化 异步处理 使用多线程处理 实现流水线处理 优化线程同步 延迟控制 减少处理帧长 优化算法复杂度 使用预测机制 实际应用中的挑战与解决方案 1. 环境噪声 问题 背景噪声干扰 非平稳噪声 突发噪声 解决方案 使用自适应阈值 多特征融合 噪声模型更新 2. 说话人差异 问题 不同说话人特征差异 说话风格变化 音量变化 解决方案 特征归一化 多模型融合 自适应参数调整 3. 实时性要求 问题 处理延迟 CPU 占用 内存使用 解决方案 算法优化 硬件加速 资源调度 总结 VAD 技术在实际应用中需要考虑多个方面：\n算法准确性 计算效率 实时性要求 环境适应性 资源消耗 通过合理的设计和优化，可以在这些方面取得良好的平衡。\n后续研究方向 深度学习应用\n端到端 VAD 模型 注意力机制 多任务学习 低资源场景\n轻量级模型 模型压缩 硬件加速 多说话人场景\n说话人分离 重叠语音检测 说话人识别 噪声环境\n鲁棒特征提取 自适应处理 噪声建模 参考文献：\nWebRTC VAD 技术文档 “Voice Activity Detection: A Review” by J. Ramirez “Digital Speech Processing” by L. Rabiner “Pattern Recognition and Machine Learning” by C. Bishop “Fundamentals of Speech Recognition” by L. Rabiner and B. Juang ","wordCount":"743","inLanguage":"en","datePublished":"2024-04-22T00:00:00Z","dateModified":"2024-04-22T00:00:00Z","author":{"@type":"Person","name":"徐亚飞"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://xuyafei.github.io/personal-site/posts/voice_activity_detection/"},"publisher":{"@type":"Organization","name":"我的博客","logo":{"@type":"ImageObject","url":"https://xuyafei.github.io/personal-site/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://xuyafei.github.io/personal-site/ accesskey=h title="我的博客 (Alt + H)">我的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://xuyafei.github.io/personal-site/categories/ title=分类><span>分类</span></a></li><li><a href=https://xuyafei.github.io/personal-site/tags/ title=标签><span>标签</span></a></li><li><a href=https://xuyafei.github.io/personal-site/archives/ title=归档><span>归档</span></a></li><li><a href=https://xuyafei.github.io/personal-site/search/ title=搜索><span>搜索</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://xuyafei.github.io/personal-site/>Home</a>&nbsp;»&nbsp;<a href=https://xuyafei.github.io/personal-site/posts/>文章</a></div><h1 class="post-title entry-hint-parent">语音活动检测（VAD）技术详解</h1><div class=post-description>深入解析语音活动检测（VAD）的原理、实现和应用，包括 WebRTC VAD 的实现细节</div><div class=post-meta><span title='2024-04-22 00:00:00 +0000 UTC'>April 22, 2024</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;徐亚飞</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e8%af%ad%e9%9f%b3%e6%b4%bb%e5%8a%a8%e6%a3%80%e6%b5%8bvad%e6%a6%82%e8%bf%b0 aria-label=语音活动检测（VAD）概述>语音活动检测（VAD）概述</a><ul><li><a href=#vad-%e7%9a%84%e4%b8%bb%e8%a6%81%e5%ba%94%e7%94%a8%e5%9c%ba%e6%99%af aria-label="VAD 的主要应用场景">VAD 的主要应用场景</a><ul><li><a href=#1-%e6%b6%88%e9%99%a4%e9%9d%99%e9%9f%b3%e6%ae%b5%e9%99%8d%e4%bd%8e%e5%b8%a6%e5%ae%bd%e5%92%8c%e8%ae%a1%e7%ae%97%e8%b5%84%e6%ba%90%e6%b6%88%e8%80%97 aria-label="1. 消除静音段，降低带宽和计算资源消耗">1. 消除静音段，降低带宽和计算资源消耗</a></li><li><a href=#2-%e9%85%8d%e5%90%88%e9%9f%b3%e9%a2%91%e5%a4%84%e7%90%86%e6%a8%a1%e5%9d%97%e4%bd%bf%e7%94%a8 aria-label="2. 配合音频处理模块使用">2. 配合音频处理模块使用</a></li><li><a href=#3-%e8%a7%a6%e5%8f%91%e8%af%ad%e9%9f%b3%e8%af%86%e5%88%abasr%e5%bc%95%e6%93%8e aria-label="3. 触发语音识别（ASR）引擎">3. 触发语音识别（ASR）引擎</a></li><li><a href=#4-%e4%bc%98%e5%8c%96%e8%af%ad%e9%9f%b3%e9%80%9a%e4%bf%a1 aria-label="4. 优化语音通信">4. 优化语音通信</a></li></ul></li><li><a href=#vad-%e7%9a%84%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86 aria-label="VAD 的基本原理">VAD 的基本原理</a><ul><li><a href=#%e7%89%b9%e5%be%81%e6%8f%90%e5%8f%96 aria-label=特征提取>特征提取</a></li><li><a href=#vad-%e7%9a%84%e7%b1%bb%e5%9e%8b aria-label="VAD 的类型">VAD 的类型</a></li></ul></li><li><a href=#webrtc-vad-%e5%ae%9e%e7%8e%b0%e8%af%a6%e8%a7%a3 aria-label="WebRTC VAD 实现详解">WebRTC VAD 实现详解</a><ul><li><a href=#%e5%a4%84%e7%90%86%e6%b5%81%e7%a8%8b aria-label=处理流程>处理流程</a></li><li><a href=#%e5%ae%9e%e7%8e%b0%e7%a4%ba%e4%be%8b aria-label=实现示例>实现示例</a></li></ul></li><li><a href=#webrtc-vad-%e7%9a%84%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86%e4%b8%8e%e6%b5%81%e7%a8%8b%e8%af%a6%e8%a7%a3 aria-label="WebRTC VAD 的数学原理与流程详解">WebRTC VAD 的数学原理与流程详解</a><ul><li><a href=#%e6%ad%a5%e9%aa%a4%e4%b8%80%e5%88%86%e5%b8%a7%e4%b8%8e%e9%a2%84%e5%a4%84%e7%90%86 aria-label=步骤一：分帧与预处理>步骤一：分帧与预处理</a></li><li><a href=#%e6%ad%a5%e9%aa%a4%e4%ba%8c%e6%bb%a4%e6%b3%a2%e5%99%a8%e7%bb%84%e5%88%86%e9%a2%91 aria-label=步骤二：滤波器组分频>步骤二：滤波器组分频</a></li><li><a href=#%e6%ad%a5%e9%aa%a4%e4%b8%89%e7%89%b9%e5%be%81%e8%ae%a1%e7%ae%97 aria-label=步骤三：特征计算>步骤三：特征计算</a></li><li><a href=#%e6%ad%a5%e9%aa%a4%e5%9b%9bgmm-%e5%88%a4%e5%86%b3%e6%a8%a1%e5%9e%8b aria-label="步骤四：GMM 判决模型">步骤四：GMM 判决模型</a></li><li><a href=#%e6%ad%a5%e9%aa%a4%e4%ba%94%e5%b9%b3%e6%bb%91%e4%b8%8e%e5%88%a4%e5%86%b3%e9%80%bb%e8%be%91 aria-label=步骤五：平滑与判决逻辑>步骤五：平滑与判决逻辑</a></li></ul></li><li><a href=#vad-%e5%9c%a8%e8%a7%86%e9%a2%91%e4%bc%9a%e8%ae%ae%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8 aria-label="VAD 在视频会议中的应用">VAD 在视频会议中的应用</a><ul><li><a href=#%e9%9f%b3%e9%a2%91%e5%a4%84%e7%90%86%e9%93%be%e8%b7%af aria-label=音频处理链路>音频处理链路</a></li><li><a href=#%e5%90%84%e9%98%b6%e6%ae%b5-vad-%e5%ba%94%e7%94%a8 aria-label="各阶段 VAD 应用">各阶段 VAD 应用</a></li></ul></li><li><a href=#%e5%ae%9e%e9%99%85%e5%ba%94%e7%94%a8%e5%bb%ba%e8%ae%ae aria-label=实际应用建议>实际应用建议</a></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a></li><li><a href=#%e5%90%8e%e7%bb%ad%e7%a0%94%e7%a9%b6%e6%96%b9%e5%90%91 aria-label=后续研究方向>后续研究方向</a></li><li><a href=#vad-%e7%9a%84%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86%e8%af%a6%e8%a7%a3 aria-label="VAD 的数学原理详解">VAD 的数学原理详解</a><ul><li><a href=#1-%e8%bf%87%e9%9b%b6%e7%8e%87zcr%e7%9a%84%e6%b7%b1%e5%85%a5%e7%90%86%e8%a7%a3 aria-label="1. 过零率（ZCR）的深入理解">1. 过零率（ZCR）的深入理解</a></li><li><a href=#2-%e9%a2%91%e8%b0%b1%e7%89%b9%e5%be%81%e5%88%86%e6%9e%90 aria-label="2. 频谱特征分析">2. 频谱特征分析</a></li></ul></li><li><a href=#gmm-%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83%e8%af%a6%e8%a7%a3 aria-label="GMM 模型训练详解">GMM 模型训练详解</a><ul><li><a href=#1-%e6%95%b0%e6%8d%ae%e5%87%86%e5%a4%87 aria-label="1. 数据准备">1. 数据准备</a></li><li><a href=#2-gmm-%e8%ae%ad%e7%bb%83%e8%bf%87%e7%a8%8b aria-label="2. GMM 训练过程">2. GMM 训练过程</a></li><li><a href=#3-%e5%ae%9e%e9%99%85%e5%ba%94%e7%94%a8%e7%a4%ba%e4%be%8b aria-label="3. 实际应用示例">3. 实际应用示例</a></li></ul></li><li><a href=#vad-%e6%80%a7%e8%83%bd%e4%bc%98%e5%8c%96 aria-label="VAD 性能优化">VAD 性能优化</a><ul><li><a href=#1-%e8%ae%a1%e7%ae%97%e4%bc%98%e5%8c%96 aria-label="1. 计算优化">1. 计算优化</a></li><li><a href=#2-%e5%86%85%e5%ad%98%e4%bc%98%e5%8c%96 aria-label="2. 内存优化">2. 内存优化</a></li><li><a href=#3-%e5%ae%9e%e6%97%b6%e6%80%a7%e4%bc%98%e5%8c%96 aria-label="3. 实时性优化">3. 实时性优化</a></li></ul></li><li><a href=#%e5%ae%9e%e9%99%85%e5%ba%94%e7%94%a8%e4%b8%ad%e7%9a%84%e6%8c%91%e6%88%98%e4%b8%8e%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88 aria-label=实际应用中的挑战与解决方案>实际应用中的挑战与解决方案</a><ul><li><a href=#1-%e7%8e%af%e5%a2%83%e5%99%aa%e5%a3%b0 aria-label="1. 环境噪声">1. 环境噪声</a></li><li><a href=#2-%e8%af%b4%e8%af%9d%e4%ba%ba%e5%b7%ae%e5%bc%82 aria-label="2. 说话人差异">2. 说话人差异</a></li><li><a href=#3-%e5%ae%9e%e6%97%b6%e6%80%a7%e8%a6%81%e6%b1%82 aria-label="3. 实时性要求">3. 实时性要求</a></li></ul></li><li><a href=#%e6%80%bb%e7%bb%93-1 aria-label=总结>总结</a></li><li><a href=#%e5%90%8e%e7%bb%ad%e7%a0%94%e7%a9%b6%e6%96%b9%e5%90%91-1 aria-label=后续研究方向>后续研究方向</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=语音活动检测vad概述>语音活动检测（VAD）概述<a hidden class=anchor aria-hidden=true href=#语音活动检测vad概述>#</a></h1><p>语音活动检测（Voice Activity Detection，VAD）是音频信号处理中的基础模块，用于判断一段音频中是否包含人声。它在实时语音通信、语音识别、音频处理等领域发挥着重要作用。</p><h2 id=vad-的主要应用场景>VAD 的主要应用场景<a hidden class=anchor aria-hidden=true href=#vad-的主要应用场景>#</a></h2><h3 id=1-消除静音段降低带宽和计算资源消耗>1. 消除静音段，降低带宽和计算资源消耗<a hidden class=anchor aria-hidden=true href=#1-消除静音段降低带宽和计算资源消耗>#</a></h3><p>在 VoIP 等实时语音通信中，VAD 可以：</p><ul><li>停止静音段的音频编码与发送</li><li>丢弃静音包或减少帧率</li><li>暂停对静音段的播放</li><li>发送 Comfort Noise（舒适噪声）包替代静音数据</li></ul><h3 id=2-配合音频处理模块使用>2. 配合音频处理模块使用<a hidden class=anchor aria-hidden=true href=#2-配合音频处理模块使用>#</a></h3><p>VAD 可以与以下模块协同工作：</p><ul><li>回声消除（AEC）：防止在静音时更新错误的回声模型</li><li>噪声抑制（NS）：帮助区分"语音+噪声"与"纯噪声"</li><li>自动增益控制（AGC）：防止对静音或噪声进行放大</li></ul><h3 id=3-触发语音识别asr引擎>3. 触发语音识别（ASR）引擎<a hidden class=anchor aria-hidden=true href=#3-触发语音识别asr引擎>#</a></h3><p>VAD 可用于：</p><ul><li>检测语音开始，启动语音识别引擎</li><li>检测语音结束，终止识别并提交结果</li><li>控制"有声录音"功能</li></ul><h3 id=4-优化语音通信>4. 优化语音通信<a hidden class=anchor aria-hidden=true href=#4-优化语音通信>#</a></h3><p>在视频会议等场景中：</p><ul><li>静音压缩（silence compression）</li><li>发送 CN（Comfort Noise）包</li><li>减少传输帧率</li></ul><h2 id=vad-的基本原理>VAD 的基本原理<a hidden class=anchor aria-hidden=true href=#vad-的基本原理>#</a></h2><h3 id=特征提取>特征提取<a hidden class=anchor aria-hidden=true href=#特征提取>#</a></h3><p>VAD 通常基于以下特征进行判断：</p><ol><li><strong>短时能量（Short-Time Energy）</strong></li></ol><pre tabindex=0><code class=language-math data-lang=math>E = \sum_{n=0}^{N-1} x[n]^2
</code></pre><ol start=2><li><strong>过零率（Zero-Crossing Rate, ZCR）</strong></li></ol><pre tabindex=0><code class=language-math data-lang=math>\text{ZCR} = \frac{1}{2N} \sum_{n=1}^{N} | \text{sgn}(x[n]) - \text{sgn}(x[n-1]) |
</code></pre><ol start=3><li><strong>谱熵（Spectral Entropy）</strong></li></ol><ul><li>衡量信号频谱的"有序性"</li><li>语音谱结构复杂，熵值较高</li></ul><ol start=4><li><strong>短时谱幅度</strong></li></ol><ul><li>语音的频谱幅度分布与噪声不同</li></ul><h3 id=vad-的类型>VAD 的类型<a hidden class=anchor aria-hidden=true href=#vad-的类型>#</a></h3><ol><li><p><strong>基于规则的传统 VAD</strong></p><ul><li>算法简单、实时性强</li><li>适合嵌入式设备</li><li>易受环境噪声影响</li></ul></li><li><p><strong>基于机器学习的 VAD</strong></p><ul><li>使用统计模型（GMM、HMM）</li><li>深度学习模型（LSTM、CNN）</li><li>更鲁棒，高噪环境下准确率高</li></ul></li><li><p><strong>集成在语音处理库中的 VAD</strong></p><ul><li>WebRTC VAD 模块</li><li>RNNoise 模块</li></ul></li></ol><h2 id=webrtc-vad-实现详解>WebRTC VAD 实现详解<a hidden class=anchor aria-hidden=true href=#webrtc-vad-实现详解>#</a></h2><h3 id=处理流程>处理流程<a hidden class=anchor aria-hidden=true href=#处理流程>#</a></h3><ol><li><p><strong>分帧与预处理</strong></p><ul><li>10ms/20ms/30ms 帧长</li><li>预加重和窗口函数处理</li></ul></li><li><p><strong>滤波器组分频</strong></p><ul><li>4个频带（80Hz~4kHz）</li><li>IIR 滤波器进行带通分离</li></ul></li><li><p><strong>特征计算</strong></p><ul><li>频带能量</li><li>谱平坦度</li><li>频谱统计量</li></ul></li><li><p><strong>GMM 判决</strong></p></li></ol><pre tabindex=0><code class=language-math data-lang=math>p(x) = \sum_{k=1}^{K} w_k \cdot \mathcal{N}(x | \mu_k, \Sigma_k)
</code></pre><ol start=5><li><strong>平滑处理</strong><ul><li>多帧平滑</li><li>动态门限调整</li><li>Hangover 机制</li></ul></li></ol><h3 id=实现示例>实现示例<a hidden class=anchor aria-hidden=true href=#实现示例>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span>VadInst<span style=color:#f92672>*</span> vad;
</span></span><span style=display:flex><span>WebRtcVad_Create(<span style=color:#f92672>&amp;</span>vad);
</span></span><span style=display:flex><span>WebRtcVad_Init(vad);
</span></span><span style=display:flex><span>WebRtcVad_set_mode(vad, <span style=color:#ae81ff>3</span>);  <span style=color:#75715e>// 模式 0~3，数字越大越敏感
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>int</span> result <span style=color:#f92672>=</span> WebRtcVad_Process(vad, <span style=color:#ae81ff>16000</span>, audio_frame, <span style=color:#ae81ff>160</span>);
</span></span></code></pre></div><h2 id=webrtc-vad-的数学原理与流程详解>WebRTC VAD 的数学原理与流程详解<a hidden class=anchor aria-hidden=true href=#webrtc-vad-的数学原理与流程详解>#</a></h2><h3 id=步骤一分帧与预处理>步骤一：分帧与预处理<a hidden class=anchor aria-hidden=true href=#步骤一分帧与预处理>#</a></h3><ol><li><p><strong>帧划分</strong></p><ul><li>输入音频被分为固定长度帧（10ms/20ms/30ms）</li><li>采样率 16kHz 时，10ms = 160 个采样点</li></ul></li><li><p><strong>预处理</strong></p><ul><li>预加重（Pre-emphasis）</li><li>窗口函数（如汉明窗）处理</li><li>增强高频特征和时域局部性</li></ul></li></ol><h3 id=步骤二滤波器组分频>步骤二：滤波器组分频<a hidden class=anchor aria-hidden=true href=#步骤二滤波器组分频>#</a></h3><ol><li><p><strong>频带划分</strong></p><ul><li>4个频带（80Hz~4kHz）</li><li>使用 IIR 滤波器进行带通分离</li><li>提取不同频段的能量特征</li></ul></li><li><p><strong>频带特征</strong></p><ul><li>低频带：80Hz~250Hz</li><li>中低频带：250Hz~1kHz</li><li>中高频带：1kHz~2kHz</li><li>高频带：2kHz~4kHz</li></ul></li></ol><h3 id=步骤三特征计算>步骤三：特征计算<a hidden class=anchor aria-hidden=true href=#步骤三特征计算>#</a></h3><ol><li><strong>频带能量计算</strong></li></ol><pre tabindex=0><code class=language-math data-lang=math>E_i = \frac{1}{N} \sum_{n=0}^{N-1} x_i[n]^2
</code></pre><ol start=2><li><strong>谱平坦度计算</strong></li></ol><pre tabindex=0><code class=language-math data-lang=math>\text{Flatness} = \frac{\text{几何均值}}{\text{算术均值}} = \frac{(\prod_{i=1}^{N} X_i)^{1/N}}{\frac{1}{N}\sum_{i=1}^{N} X_i}
</code></pre><ol start=3><li><strong>频谱统计量</strong><ul><li>最大频率分量</li><li>频谱质心</li><li>频谱带宽</li></ul></li></ol><h3 id=步骤四gmm-判决模型>步骤四：GMM 判决模型<a hidden class=anchor aria-hidden=true href=#步骤四gmm-判决模型>#</a></h3><ol><li><p><strong>GMM 模型结构</strong></p><ul><li>4个语音活动水平类别</li><li>每类使用2个高斯分量</li><li>特征维度为6</li></ul></li><li><p><strong>概率计算</strong></p></li></ol><pre tabindex=0><code class=language-math data-lang=math>\log p(x|C_k) = \log \sum_{i=1}^M w_{k,i} \cdot \mathcal{N}(x; \mu_{k,i}, \Sigma_{k,i})
</code></pre><ol start=3><li><strong>类别判决</strong></li></ol><pre tabindex=0><code class=language-math data-lang=math>\hat{k} = \arg \max_k \log p(x|C_k)
</code></pre><h3 id=步骤五平滑与判决逻辑>步骤五：平滑与判决逻辑<a hidden class=anchor aria-hidden=true href=#步骤五平滑与判决逻辑>#</a></h3><ol><li><p><strong>多帧平滑</strong></p><ul><li>3帧或5帧投票法</li><li>连续多帧为语音才判定为说话开始</li></ul></li><li><p><strong>动态门限调整</strong></p><ul><li>根据语音能量动态调整阈值</li><li>适应环境变化</li></ul></li><li><p><strong>Hangover 机制</strong></p><ul><li>说话结束时保持几帧语音状态</li><li>防止语尾被截断</li></ul></li></ol><h2 id=vad-在视频会议中的应用>VAD 在视频会议中的应用<a hidden class=anchor aria-hidden=true href=#vad-在视频会议中的应用>#</a></h2><h3 id=音频处理链路>音频处理链路<a hidden class=anchor aria-hidden=true href=#音频处理链路>#</a></h3><pre tabindex=0><code>麦克风输入
   ↓
前处理：高通滤波 / 去直流偏移
   ↓
► AEC（回声消除）
   ↓
► NS（噪声抑制）
   ↓
► AGC（自动增益控制）
   ↓
► VAD（语音活动检测）
   ↓
编码器（Opus / AAC 等）
   ↓
网络传输（RTP / SRTP）
</code></pre><h3 id=各阶段-vad-应用>各阶段 VAD 应用<a hidden class=anchor aria-hidden=true href=#各阶段-vad-应用>#</a></h3><ol><li><p><strong>与 AEC 协作</strong></p><ul><li>防止静音时更新错误回声模型</li><li>避免背景噪声被当作回声源</li></ul></li><li><p><strong>与 NS 协作</strong></p><ul><li>区分"语音+噪声"与"纯噪声"</li><li>更新噪声模型</li></ul></li><li><p><strong>与 AGC 协作</strong></p><ul><li>防止对静音或噪声放大</li><li>提升听感体验</li></ul></li><li><p><strong>编码器控制</strong></p><ul><li>判断是否需要编码</li><li>控制静音压缩模式</li><li>生成舒适噪声（CN）</li></ul></li><li><p><strong>网络传输控制</strong></p><ul><li>控制 RTP 包发送</li><li>调整发送频率</li></ul></li></ol><h2 id=实际应用建议>实际应用建议<a hidden class=anchor aria-hidden=true href=#实际应用建议>#</a></h2><ol><li><p><strong>部署位置</strong></p><ul><li>音频前处理后的某一帧处理阶段</li><li>编码前和网络传输控制前</li></ul></li><li><p><strong>参数调优</strong></p><ul><li>根据实际场景选择合适的灵敏度模式</li><li>调整平滑参数减少误判</li></ul></li><li><p><strong>性能优化</strong></p><ul><li>异步处理避免阻塞</li><li>合理设置帧长和缓冲区</li></ul></li></ol><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>VAD 是音频处理中的重要基础模块，通过合理使用 VAD，可以：</p><ul><li>降低带宽和计算资源消耗</li><li>提高音频处理质量</li><li>优化语音识别效果</li><li>改善实时通信体验</li></ul><h2 id=后续研究方向>后续研究方向<a hidden class=anchor aria-hidden=true href=#后续研究方向>#</a></h2><ol><li>深度学习在 VAD 中的应用</li><li>低延迟 VAD 算法研究</li><li>多说话人场景的 VAD</li><li>噪声环境下的鲁棒性提升</li></ol><h2 id=vad-的数学原理详解>VAD 的数学原理详解<a hidden class=anchor aria-hidden=true href=#vad-的数学原理详解>#</a></h2><h3 id=1-过零率zcr的深入理解>1. 过零率（ZCR）的深入理解<a hidden class=anchor aria-hidden=true href=#1-过零率zcr的深入理解>#</a></h3><p>过零率是语音信号分析中最经典的特征之一，它能反映信号的频率特性：</p><ol><li><strong>数学定义</strong></li></ol><pre tabindex=0><code class=language-math data-lang=math>\text{ZCR} = \frac{1}{2N} \sum_{n=1}^{N} | \text{sgn}(x[n]) - \text{sgn}(x[n-1]) |
</code></pre><ol start=2><li><strong>物理意义</strong></li></ol><ul><li>高频信号：起伏快，频繁过零（ZCR高）</li><li>低频信号：起伏慢，过零次数少（ZCR低）</li></ul><ol start=3><li><strong>在 VAD 中的应用</strong></li></ol><ul><li>结合能量特征使用：<ul><li>能量低 + ZCR高 → 可能是噪声</li><li>能量高 + ZCR低 → 多数是语音（尤其是元音）</li><li>能量高 + ZCR高 → 咝音/辅音/语音边缘</li></ul></li></ul><h3 id=2-频谱特征分析>2. 频谱特征分析<a hidden class=anchor aria-hidden=true href=#2-频谱特征分析>#</a></h3><ol><li><strong>短时傅里叶变换（STFT）</strong></li></ol><pre tabindex=0><code class=language-math data-lang=math>X(k) = \sum_{n=0}^{N-1} x(n)w(n)e^{-j2\pi kn/N}
</code></pre><ol start=2><li><strong>谱熵计算</strong></li></ol><pre tabindex=0><code class=language-math data-lang=math>H = -\sum_{i=1}^{N} p_i \log_2(p_i)
</code></pre><p>其中 $p_i$ 是第 i 个频带的归一化能量。</p><ol start=3><li><strong>频谱平坦度</strong></li></ol><pre tabindex=0><code class=language-math data-lang=math>\text{Flatness} = \frac{\sqrt[N]{\prod_{i=1}^{N} X_i}}{\frac{1}{N}\sum_{i=1}^{N} X_i}
</code></pre><h2 id=gmm-模型训练详解>GMM 模型训练详解<a hidden class=anchor aria-hidden=true href=#gmm-模型训练详解>#</a></h2><h3 id=1-数据准备>1. 数据准备<a hidden class=anchor aria-hidden=true href=#1-数据准备>#</a></h3><ol><li><strong>训练数据收集</strong></li></ol><ul><li>语音数据：包含各种说话人、语速、音量的语音</li><li>非语音数据：包含各种环境噪声、背景音</li><li>数据标注：人工标注语音/非语音段</li></ul><ol start=2><li><strong>特征提取</strong></li></ol><ul><li>提取 6 维特征向量：<ul><li>低频能量</li><li>中频能量</li><li>高频能量</li><li>能量比例</li><li>谱差</li><li>谱平坦度</li></ul></li></ul><h3 id=2-gmm-训练过程>2. GMM 训练过程<a hidden class=anchor aria-hidden=true href=#2-gmm-训练过程>#</a></h3><ol><li><strong>模型初始化</strong></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 示例代码</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.mixture <span style=color:#f92672>import</span> GaussianMixture
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 初始化 GMM</span>
</span></span><span style=display:flex><span>gmm <span style=color:#f92672>=</span> GaussianMixture(
</span></span><span style=display:flex><span>    n_components<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>,  <span style=color:#75715e># 每类使用2个高斯分量</span>
</span></span><span style=display:flex><span>    covariance_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;diag&#39;</span>,  <span style=color:#75715e># 使用对角协方差矩阵</span>
</span></span><span style=display:flex><span>    random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 训练模型</span>
</span></span><span style=display:flex><span>gmm<span style=color:#f92672>.</span>fit(X_train)  <span style=color:#75715e># X_train 是特征矩阵</span>
</span></span></code></pre></div><ol start=2><li><strong>EM 算法迭代</strong></li></ol><ul><li>E 步：计算每个样本属于各个高斯分量的概率</li><li>M 步：更新模型参数（均值、协方差、权重）</li></ul><ol start=3><li><strong>模型评估</strong></li></ol><ul><li>使用验证集评估模型性能</li><li>调整模型参数（如高斯分量数量）</li></ul><h3 id=3-实际应用示例>3. 实际应用示例<a hidden class=anchor aria-hidden=true href=#3-实际应用示例>#</a></h3><ol><li><strong>Python 实现示例</strong></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.mixture <span style=color:#f92672>import</span> GaussianMixture
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>extract_features</span>(audio_frame, sample_rate):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 1. 分帧</span>
</span></span><span style=display:flex><span>    frame_length <span style=color:#f92672>=</span> int(<span style=color:#ae81ff>0.02</span> <span style=color:#f92672>*</span> sample_rate)  <span style=color:#75715e># 20ms</span>
</span></span><span style=display:flex><span>    frames <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array_split(audio_frame, len(audio_frame) <span style=color:#f92672>//</span> frame_length)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    features <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> frame <span style=color:#f92672>in</span> frames:
</span></span><span style=display:flex><span>        <span style=color:#75715e># 2. 计算频谱</span>
</span></span><span style=display:flex><span>        spectrum <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>abs(np<span style=color:#f92672>.</span>fft<span style=color:#f92672>.</span>rfft(frame))
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 3. 提取特征</span>
</span></span><span style=display:flex><span>        energy <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum(frame <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        zcr <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum(np<span style=color:#f92672>.</span>abs(np<span style=color:#f92672>.</span>diff(np<span style=color:#f92672>.</span>signbit(frame))))
</span></span><span style=display:flex><span>        spectral_entropy <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>np<span style=color:#f92672>.</span>sum((spectrum<span style=color:#f92672>/</span>np<span style=color:#f92672>.</span>sum(spectrum)) <span style=color:#f92672>*</span> 
</span></span><span style=display:flex><span>                                 np<span style=color:#f92672>.</span>log2(spectrum<span style=color:#f92672>/</span>np<span style=color:#f92672>.</span>sum(spectrum) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-10</span>))
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        features<span style=color:#f92672>.</span>append([energy, zcr, spectral_entropy])
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>array(features)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>vad_detection</span>(audio_frame, sample_rate, gmm_model):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 1. 特征提取</span>
</span></span><span style=display:flex><span>    features <span style=color:#f92672>=</span> extract_features(audio_frame, sample_rate)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 2. GMM 预测</span>
</span></span><span style=display:flex><span>    log_probs <span style=color:#f92672>=</span> gmm_model<span style=color:#f92672>.</span>score_samples(features)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 3. 判决</span>
</span></span><span style=display:flex><span>    is_speech <span style=color:#f92672>=</span> log_probs <span style=color:#f92672>&gt;</span> threshold
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> is_speech
</span></span></code></pre></div><ol start=2><li><strong>C++ 实现示例（WebRTC 风格）</strong></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>VAD</span> {
</span></span><span style=display:flex><span><span style=color:#66d9ef>public</span><span style=color:#f92672>:</span>
</span></span><span style=display:flex><span>    VAD() {
</span></span><span style=display:flex><span>        <span style=color:#75715e>// 初始化 GMM 参数
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        initGMMParams();
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>bool</span> <span style=color:#a6e22e>ProcessFrame</span>(<span style=color:#66d9ef>const</span> <span style=color:#66d9ef>int16_t</span><span style=color:#f92672>*</span> audio_frame, <span style=color:#66d9ef>int</span> frame_length) {
</span></span><span style=display:flex><span>        <span style=color:#75715e>// 1. 特征提取
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        std<span style=color:#f92672>::</span>vector<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>float</span><span style=color:#f92672>&gt;</span> features <span style=color:#f92672>=</span> ExtractFeatures(audio_frame, frame_length);
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e>// 2. GMM 计算
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#66d9ef>float</span> log_prob <span style=color:#f92672>=</span> ComputeGMMProbability(features);
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e>// 3. 判决
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#66d9ef>return</span> log_prob <span style=color:#f92672>&gt;</span> threshold_;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#66d9ef>private</span><span style=color:#f92672>:</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>void</span> initGMMParams() {
</span></span><span style=display:flex><span>        <span style=color:#75715e>// 初始化 GMM 参数（均值、方差、权重）
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#75715e>// 这些参数通常是预训练好的
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    }
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    std<span style=color:#f92672>::</span>vector<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>float</span><span style=color:#f92672>&gt;</span> ExtractFeatures(<span style=color:#66d9ef>const</span> <span style=color:#66d9ef>int16_t</span><span style=color:#f92672>*</span> frame, <span style=color:#66d9ef>int</span> length) {
</span></span><span style=display:flex><span>        <span style=color:#75715e>// 实现特征提取
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#75715e>// 返回特征向量
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    }
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>float</span> <span style=color:#a6e22e>ComputeGMMProbability</span>(<span style=color:#66d9ef>const</span> std<span style=color:#f92672>::</span>vector<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>float</span><span style=color:#f92672>&gt;&amp;</span> features) {
</span></span><span style=display:flex><span>        <span style=color:#75715e>// 实现 GMM 概率计算
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#75715e>// 返回对数概率
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    }
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>float</span> threshold_;
</span></span><span style=display:flex><span>    <span style=color:#75715e>// GMM 参数
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    std<span style=color:#f92672>::</span>vector<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>float</span><span style=color:#f92672>&gt;</span> means_;
</span></span><span style=display:flex><span>    std<span style=color:#f92672>::</span>vector<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>float</span><span style=color:#f92672>&gt;</span> variances_;
</span></span><span style=display:flex><span>    std<span style=color:#f92672>::</span>vector<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>float</span><span style=color:#f92672>&gt;</span> weights_;
</span></span><span style=display:flex><span>};
</span></span></code></pre></div><h2 id=vad-性能优化>VAD 性能优化<a hidden class=anchor aria-hidden=true href=#vad-性能优化>#</a></h2><h3 id=1-计算优化>1. 计算优化<a hidden class=anchor aria-hidden=true href=#1-计算优化>#</a></h3><ol><li><strong>特征计算优化</strong></li></ol><ul><li>使用 SIMD 指令加速</li><li>预计算常用值</li><li>使用查找表代替复杂计算</li></ul><ol start=2><li><strong>GMM 计算优化</strong></li></ol><ul><li>使用定点数计算</li><li>简化协方差矩阵（使用对角矩阵）</li><li>预计算常用值</li></ul><h3 id=2-内存优化>2. 内存优化<a hidden class=anchor aria-hidden=true href=#2-内存优化>#</a></h3><ol><li><strong>缓冲区管理</strong></li></ol><ul><li>使用循环缓冲区</li><li>避免频繁内存分配</li><li>合理设置缓冲区大小</li></ul><ol start=2><li><strong>参数存储</strong></li></ol><ul><li>使用定点数存储模型参数</li><li>压缩存储模型参数</li><li>共享常用参数</li></ul><h3 id=3-实时性优化>3. 实时性优化<a hidden class=anchor aria-hidden=true href=#3-实时性优化>#</a></h3><ol><li><strong>异步处理</strong></li></ol><ul><li>使用多线程处理</li><li>实现流水线处理</li><li>优化线程同步</li></ul><ol start=2><li><strong>延迟控制</strong></li></ol><ul><li>减少处理帧长</li><li>优化算法复杂度</li><li>使用预测机制</li></ul><h2 id=实际应用中的挑战与解决方案>实际应用中的挑战与解决方案<a hidden class=anchor aria-hidden=true href=#实际应用中的挑战与解决方案>#</a></h2><h3 id=1-环境噪声>1. 环境噪声<a hidden class=anchor aria-hidden=true href=#1-环境噪声>#</a></h3><ol><li><strong>问题</strong></li></ol><ul><li>背景噪声干扰</li><li>非平稳噪声</li><li>突发噪声</li></ul><ol start=2><li><strong>解决方案</strong></li></ol><ul><li>使用自适应阈值</li><li>多特征融合</li><li>噪声模型更新</li></ul><h3 id=2-说话人差异>2. 说话人差异<a hidden class=anchor aria-hidden=true href=#2-说话人差异>#</a></h3><ol><li><strong>问题</strong></li></ol><ul><li>不同说话人特征差异</li><li>说话风格变化</li><li>音量变化</li></ul><ol start=2><li><strong>解决方案</strong></li></ol><ul><li>特征归一化</li><li>多模型融合</li><li>自适应参数调整</li></ul><h3 id=3-实时性要求>3. 实时性要求<a hidden class=anchor aria-hidden=true href=#3-实时性要求>#</a></h3><ol><li><strong>问题</strong></li></ol><ul><li>处理延迟</li><li>CPU 占用</li><li>内存使用</li></ul><ol start=2><li><strong>解决方案</strong></li></ol><ul><li>算法优化</li><li>硬件加速</li><li>资源调度</li></ul><h2 id=总结-1>总结<a hidden class=anchor aria-hidden=true href=#总结-1>#</a></h2><p>VAD 技术在实际应用中需要考虑多个方面：</p><ol><li>算法准确性</li><li>计算效率</li><li>实时性要求</li><li>环境适应性</li><li>资源消耗</li></ol><p>通过合理的设计和优化，可以在这些方面取得良好的平衡。</p><h2 id=后续研究方向-1>后续研究方向<a hidden class=anchor aria-hidden=true href=#后续研究方向-1>#</a></h2><ol><li><p><strong>深度学习应用</strong></p><ul><li>端到端 VAD 模型</li><li>注意力机制</li><li>多任务学习</li></ul></li><li><p><strong>低资源场景</strong></p><ul><li>轻量级模型</li><li>模型压缩</li><li>硬件加速</li></ul></li><li><p><strong>多说话人场景</strong></p><ul><li>说话人分离</li><li>重叠语音检测</li><li>说话人识别</li></ul></li><li><p><strong>噪声环境</strong></p><ul><li>鲁棒特征提取</li><li>自适应处理</li><li>噪声建模</li></ul></li></ol><hr><p><em>参考文献：</em></p><ol><li>WebRTC VAD 技术文档</li><li>&ldquo;Voice Activity Detection: A Review&rdquo; by J. Ramirez</li><li>&ldquo;Digital Speech Processing&rdquo; by L. Rabiner</li><li>&ldquo;Pattern Recognition and Machine Learning&rdquo; by C. Bishop</li><li>&ldquo;Fundamentals of Speech Recognition&rdquo; by L. Rabiner and B. Juang</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://xuyafei.github.io/personal-site/tags/%E9%9F%B3%E9%A2%91%E5%A4%84%E7%90%86/>音频处理</a></li><li><a href=https://xuyafei.github.io/personal-site/tags/vad/>VAD</a></li><li><a href=https://xuyafei.github.io/personal-site/tags/webrtc/>WebRTC</a></li><li><a href=https://xuyafei.github.io/personal-site/tags/%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/>信号处理</a></li><li><a href=https://xuyafei.github.io/personal-site/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></li></ul><nav class=paginav><a class=prev href=https://xuyafei.github.io/personal-site/posts/webrtc_aec_rnnoise/><span class=title>« Prev</span><br><span>WebRTC AEC 与 RNNoise 组合：增强回声消除效果的技术方案</span>
</a><a class=next href=https://xuyafei.github.io/personal-site/posts/scalable_video_coding/><span class=title>Next »</span><br><span>可伸缩视频编码（SVC）技术详解</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 语音活动检测（VAD）技术详解 on x" href="https://x.com/intent/tweet/?text=%e8%af%ad%e9%9f%b3%e6%b4%bb%e5%8a%a8%e6%a3%80%e6%b5%8b%ef%bc%88VAD%ef%bc%89%e6%8a%80%e6%9c%af%e8%af%a6%e8%a7%a3&amp;url=https%3a%2f%2fxuyafei.github.io%2fpersonal-site%2fposts%2fvoice_activity_detection%2f&amp;hashtags=%e9%9f%b3%e9%a2%91%e5%a4%84%e7%90%86%2cVAD%2cWebRTC%2c%e4%bf%a1%e5%8f%b7%e5%a4%84%e7%90%86%2c%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 语音活动检测（VAD）技术详解 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fxuyafei.github.io%2fpersonal-site%2fposts%2fvoice_activity_detection%2f&amp;title=%e8%af%ad%e9%9f%b3%e6%b4%bb%e5%8a%a8%e6%a3%80%e6%b5%8b%ef%bc%88VAD%ef%bc%89%e6%8a%80%e6%9c%af%e8%af%a6%e8%a7%a3&amp;summary=%e8%af%ad%e9%9f%b3%e6%b4%bb%e5%8a%a8%e6%a3%80%e6%b5%8b%ef%bc%88VAD%ef%bc%89%e6%8a%80%e6%9c%af%e8%af%a6%e8%a7%a3&amp;source=https%3a%2f%2fxuyafei.github.io%2fpersonal-site%2fposts%2fvoice_activity_detection%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 语音活动检测（VAD）技术详解 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fxuyafei.github.io%2fpersonal-site%2fposts%2fvoice_activity_detection%2f&title=%e8%af%ad%e9%9f%b3%e6%b4%bb%e5%8a%a8%e6%a3%80%e6%b5%8b%ef%bc%88VAD%ef%bc%89%e6%8a%80%e6%9c%af%e8%af%a6%e8%a7%a3"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 语音活动检测（VAD）技术详解 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fxuyafei.github.io%2fpersonal-site%2fposts%2fvoice_activity_detection%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 语音活动检测（VAD）技术详解 on whatsapp" href="https://api.whatsapp.com/send?text=%e8%af%ad%e9%9f%b3%e6%b4%bb%e5%8a%a8%e6%a3%80%e6%b5%8b%ef%bc%88VAD%ef%bc%89%e6%8a%80%e6%9c%af%e8%af%a6%e8%a7%a3%20-%20https%3a%2f%2fxuyafei.github.io%2fpersonal-site%2fposts%2fvoice_activity_detection%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 语音活动检测（VAD）技术详解 on telegram" href="https://telegram.me/share/url?text=%e8%af%ad%e9%9f%b3%e6%b4%bb%e5%8a%a8%e6%a3%80%e6%b5%8b%ef%bc%88VAD%ef%bc%89%e6%8a%80%e6%9c%af%e8%af%a6%e8%a7%a3&amp;url=https%3a%2f%2fxuyafei.github.io%2fpersonal-site%2fposts%2fvoice_activity_detection%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 语音活动检测（VAD）技术详解 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e8%af%ad%e9%9f%b3%e6%b4%bb%e5%8a%a8%e6%a3%80%e6%b5%8b%ef%bc%88VAD%ef%bc%89%e6%8a%80%e6%9c%af%e8%af%a6%e8%a7%a3&u=https%3a%2f%2fxuyafei.github.io%2fpersonal-site%2fposts%2fvoice_activity_detection%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://xuyafei.github.io/personal-site/>我的博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
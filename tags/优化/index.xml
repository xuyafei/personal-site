<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>优化 on 我的博客</title><link>https://xuyafei.github.io/personal-site/tags/%E4%BC%98%E5%8C%96/</link><description>Recent content in 优化 on 我的博客</description><generator>Hugo -- 0.147.0</generator><language>zh-cn</language><lastBuildDate>Mon, 22 Apr 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://xuyafei.github.io/personal-site/tags/%E4%BC%98%E5%8C%96/index.xml" rel="self" type="application/rss+xml"/><item><title>理解损失函数：机器学习中不可或缺的关键</title><link>https://xuyafei.github.io/personal-site/posts/loss-function/</link><pubDate>Mon, 22 Apr 2024 00:00:00 +0000</pubDate><guid>https://xuyafei.github.io/personal-site/posts/loss-function/</guid><description>深入解析四种常见损失函数（MSE、MAE、交叉熵、Hinge Loss）的数学原理、特点及应用场景，帮助你全面理解损失函数在机器学习中的重要作用。</description></item><item><title>梯度优化初步理解与推导</title><link>https://xuyafei.github.io/personal-site/posts/preliminary-understanding-and-derivation-of-gradient-optimization/</link><pubDate>Thu, 21 Mar 2024 00:00:00 +0000</pubDate><guid>https://xuyafei.github.io/personal-site/posts/preliminary-understanding-and-derivation-of-gradient-optimization/</guid><description>从单变量到多变量函数，深入浅出地讲解梯度优化原理，包含数学推导与直观解释</description></item></channel></rss>